{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNECjXYNAPYXAFtjD5/tsn0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c11546515c284c7c8a9d1208f96b8cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e6fc452d782436c8c111336e998c403",
              "IPY_MODEL_3fd0d2b6538b4a91adc0dd3258ba26d7",
              "IPY_MODEL_d4d75b9bdcb14d38a9f16395f757945d"
            ],
            "layout": "IPY_MODEL_211e34461c13410081716b76d5a769b1"
          }
        },
        "6e6fc452d782436c8c111336e998c403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76858053986e495cabe59f59fa8aaf89",
            "placeholder": "​",
            "style": "IPY_MODEL_d24ee2d7d2de407aa984e4e774bbf688",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3fd0d2b6538b4a91adc0dd3258ba26d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_796cba14102b4cdbaa8e86e58d114ef8",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35aea6045053449fa9b0b594ba39030d",
            "value": 4
          }
        },
        "d4d75b9bdcb14d38a9f16395f757945d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f6f3e53555e4a2baf17e67816c86d8f",
            "placeholder": "​",
            "style": "IPY_MODEL_7f3598cd079e481ca6f2f7b245ae81d3",
            "value": " 4/4 [01:10&lt;00:00, 15.25s/it]"
          }
        },
        "211e34461c13410081716b76d5a769b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76858053986e495cabe59f59fa8aaf89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24ee2d7d2de407aa984e4e774bbf688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "796cba14102b4cdbaa8e86e58d114ef8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35aea6045053449fa9b0b594ba39030d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f6f3e53555e4a2baf17e67816c86d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f3598cd079e481ca6f2f7b245ae81d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainiakhil/AI-receptionist-for-Doctor/blob/main/AI_receptionist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kgh5NCfybzeu"
      },
      "outputs": [],
      "source": [
        "!pip install transformers qdrant-client aiohttp asyncio bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "ZdIz2oUmcAne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCqoPeAW9JTq",
        "outputId": "a6d49b11-3524-4e94-b06f-6945b2fec71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import BitsAndBytesConfig # Import the BitsAndBytesConfig class\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import asyncio\n",
        "import random\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "4jl3uojKcFNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LLM from Hugging Face\n",
        "\n",
        "\n",
        "llm = pipeline(\"text-generation\", model=\"NousResearch/Meta-Llama-3.1-8B\")\n",
        "llm.to('cuda')\n",
        "\n",
        "# Initialize the embedding model (e.g., SentenceTransformer)\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235,
          "referenced_widgets": [
            "c11546515c284c7c8a9d1208f96b8cc3",
            "6e6fc452d782436c8c111336e998c403",
            "3fd0d2b6538b4a91adc0dd3258ba26d7",
            "d4d75b9bdcb14d38a9f16395f757945d",
            "211e34461c13410081716b76d5a769b1",
            "76858053986e495cabe59f59fa8aaf89",
            "d24ee2d7d2de407aa984e4e774bbf688",
            "796cba14102b4cdbaa8e86e58d114ef8",
            "35aea6045053449fa9b0b594ba39030d",
            "8f6f3e53555e4a2baf17e67816c86d8f",
            "7f3598cd079e481ca6f2f7b245ae81d3"
          ]
        },
        "id": "BPjCcry8cV3h",
        "outputId": "f5daa9a2-6fc1-4361-d9a8-3e918e259702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c11546515c284c7c8a9d1208f96b8cc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def populate_emergency_responses():\n",
        "\n",
        "    emergency_responses = {\n",
        "            \"not breathing\": \"Start CPR immediately. Push hard and fast in the center of the chest and provide rescue breaths.\",\n",
        "            \"chest pain\": \"Stay calm and take slow, deep breaths. Sit in a comfortable position while waiting for help.\"\n",
        "        }\n",
        "\n",
        "      # Encode the descriptions into vectors\n",
        "    descriptions = list(emergency_responses.keys())\n",
        "    vectors = model.encode(descriptions)\n",
        "\n",
        "      # Initialize FAISS index\n",
        "    dimension = vectors.shape[1]  # Dimension of the vectors\n",
        "    index = faiss.IndexFlatL2(dimension)  # L2 distance metric\n",
        "\n",
        "      # Add vectors to the FAISS index\n",
        "    index.add(np.array(vectors))\n",
        "\n",
        "      # Optional: Keep track of the responses\n",
        "    response_map = {i: emergency_responses[desc] for i, desc in enumerate(descriptions)}\n",
        "\n",
        "    return index, response_map"
      ],
      "metadata": {
        "id": "jcnFJBOkclAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db, response_map = populate_emergency_responses()"
      ],
      "metadata": {
        "id": "kCunme0ST754"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the vector database asynchronously\n",
        "\n",
        "async def query_vector_db(emergency_description):\n",
        "    #await asyncio.sleep(15)  # Simulate 15 seconds delay\n",
        "    query_vector = model.encode([emergency_description])\n",
        "\n",
        "    # Perform the search\n",
        "    k = 1  # Number of nearest neighbors\n",
        "    distances, indices = db.search(query_vector, k)\n",
        "\n",
        "    if indices is None or len(indices) == 0:\n",
        "        return \"Sorry, I don't have specific instructions for that emergency.\"\n",
        "\n",
        "    else:\n",
        "        return response_map[indices[0][0]]\n"
      ],
      "metadata": {
        "id": "qVQIj4Ngcqsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate response using LLM asynchronously\n",
        "async def generate_response(prompt):\n",
        "    return llm(prompt, max_length=50)[0]['generated_text']\n",
        "\n",
        "# Main AI receptionist logic\n",
        "async def ai_receptionist():\n",
        "    while True:\n",
        "        # Step 1: Confirm emergency or message\n",
        "        response = await generate_response(\"Are you having an emergency, or would you like to leave a message for Dr. Adrin?\")\n",
        "        print(response)\n",
        "\n",
        "        user_input = input().strip().lower()\n",
        "\n",
        "        if \"message\" in user_input:\n",
        "            # Step 2: Handle message\n",
        "            message_prompt = \"Please type your message:\"\n",
        "            message_response = await generate_response(message_prompt)\n",
        "            print(message_response)\n",
        "\n",
        "            user_message = input()\n",
        "            thanks_response = \"Thanks for the message, we will forward it to Dr. Adrin.\"\n",
        "            print(thanks_response)\n",
        "            break\n",
        "        elif \"emergency\" in user_input:\n",
        "            # Step 3: Handle emergency\n",
        "            emergency_prompt = \"Please describe the emergency:\"\n",
        "            emergency_description = input(emergency_prompt).strip().lower()\n",
        "\n",
        "            # Async task to query vector database\n",
        "            db_task = asyncio.create_task(query_vector_db(emergency_description))\n",
        "\n",
        "            # Step 4: Continue conversation while waiting\n",
        "            location_prompt = \"I am checking what you should do immediately, meanwhile, can you tell me which area you are located in right now?\"\n",
        "            location_response = await generate_response(location_prompt)\n",
        "            print(location_response)\n",
        "\n",
        "            location = input().strip().lower()\n",
        "\n",
        "            # Wait for the vector database response (with a delay)\n",
        "            emergency_instructions = await db_task\n",
        "\n",
        "            # Step 5: Provide ETA and handle urgency\n",
        "            eta = random.randint(5, 20)  # Generate random ETA in minutes\n",
        "            eta_response = f\"Dr. Adrin will be coming to your location in {eta} minutes.\"\n",
        "            print(eta_response)\n",
        "\n",
        "            too_late_input = input(\"If you think the arrival will be too late, type 'too late': \").strip().lower()\n",
        "            if \"too late\" in too_late_input:\n",
        "                too_late_response = f\"I understand that you are worried that Dr. Adrin will arrive too late. Meanwhile, {emergency_instructions}\"\n",
        "                print(too_late_response)\n",
        "            else:\n",
        "                final_response = \"Don’t worry, please follow these steps, Dr. Adrin will be with you shortly.\"\n",
        "                print(final_response)\n",
        "            break\n",
        "        else:\n",
        "            # Handle unrelated topics\n",
        "            misunderstanding_response = \"I don’t understand that. Let's try again.\"\n",
        "            print(misunderstanding_response)\n"
      ],
      "metadata": {
        "id": "qrOI_qfkcsUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the AI receptionist\n",
        "async def main():\n",
        "\n",
        "    # Start the AI receptionist\n",
        "    await ai_receptionist()\n",
        "\n",
        "#asyncio.run(main())\n",
        "# Instead of asyncio.run(main()), just use:\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfi9K4VUcwJY",
        "outputId": "988589c4-4907-4200-c8af-895c1a195240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Are you having an emergency, or would you like to leave a message for Dr. Adrin? Please call our office at (360) 734-4400. We will return your call as soon as possible.\n",
            "emergency\n",
            "Please describe the emergency:not breathing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am checking what you should do immediately, meanwhile, can you tell me which area you are located in right now? Also, are you in a shelter or with family/friends?\n",
            "I am in the Bay Area. I am at home,\n",
            "I am in shyam nagar\n",
            "Dr. Adrin will be coming to your location in 13 minutes.\n",
            "If you think the arrival will be too late, type 'too late': too late\n",
            "I understand that you are worried that Dr. Adrin will arrive too late. Meanwhile, Start CPR immediately. Push hard and fast in the center of the chest and provide rescue breaths.\n"
          ]
        }
      ]
    }
  ]
}